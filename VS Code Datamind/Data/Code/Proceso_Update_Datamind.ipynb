{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liberia\n",
    "import pandas as pd\n",
    "\n",
    "#Permite buscar y recuperar una lista de nombres de archivos que coinciden con un patrón específico de nombre\n",
    "#de archivo en un directorio o en una jerarquía de directorios.\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "#import dask.dataframe as dd\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ruta_chile_men= r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Actualizacion\\Actualizacion Chile Mensual.xlsx'\n",
    "ruta_chile_sem =r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Actualizacion\\Actualizacion Chile Semanal.xlsx'\n",
    "\n",
    "ruta_arg_men =r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Actualizacion\\Actualizacion Argentina Mensual.xlsx'\n",
    "ruta_arg_sem =r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Actualizacion\\Actualizacion Argentina Semanal.xlsx'\n",
    "\n",
    "ruta_per_men = r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Actualizacion\\Actualizacion Peru Mensual.xlsx'\n",
    "ruta_per_sem = r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Actualizacion\\Actualizacion Peru Semanal.xlsx'\n",
    "\n",
    "ruta_mex_men = r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Actualizacion\\Actualizacion Mexico  Mensual.xlsx'\n",
    "ruta_mex_sem = r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Actualizacion\\Actualizacion Mexico Semanal.xlsx'\n",
    "\n",
    "\n",
    "df_chi_men = pd.read_excel(ruta_chile_men, header=0,dtype=str)\n",
    "df_arg_men = pd.read_excel(ruta_arg_men, header=0,dtype=str)\n",
    "df_per_men = pd.read_excel(ruta_per_men, header=0,dtype=str)\n",
    "df_mex_men = pd.read_excel(ruta_mex_men, header=0,dtype=str)\n",
    "\n",
    "df_chi_sem = pd.read_excel(ruta_chile_sem, header=0,dtype=str)\n",
    "df_arg_sem = pd.read_excel(ruta_arg_sem, header=0,dtype=str)\n",
    "df_per_sem = pd.read_excel(ruta_per_sem, header=0,dtype=str)\n",
    "df_mex_sem= pd.read_excel(ruta_mex_sem, header=0,dtype=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- se grega col tipo de datos ----------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "df_chi_men['Tipo de dato'] ='men'\n",
    "df_mex_men['Tipo de dato'] ='men'\n",
    "df_arg_men['Tipo de dato'] ='men'\n",
    "df_per_men['Tipo de dato'] ='men'\n",
    "\n",
    "\n",
    "df_chi_sem['Tipo de dato'] ='sem'\n",
    "df_mex_sem['Tipo de dato'] ='sem'\n",
    "df_arg_sem['Tipo de dato'] ='sem'\n",
    "df_per_sem['Tipo de dato'] ='sem'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- rename columna ----------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "df_chi_men.rename(columns={'Mes':'Date'},inplace=True)\n",
    "df_chi_sem.rename(columns={'Semana':'Date'},inplace=True)\n",
    "\n",
    "df_arg_men.rename(columns={'Mes': 'Date'},inplace=True)\n",
    "df_arg_sem.rename(columns={'Semana':'Date'},inplace=True)\n",
    "\n",
    "df_per_men.rename(columns={'Mes':'Date'},inplace=True)\n",
    "df_per_sem.rename(columns={'Semana':'Date'},inplace=True)\n",
    "\n",
    "df_mex_men.rename(columns={'Mes':'Date'},inplace=True)\n",
    "df_mex_sem.rename(columns={'Semana':'Date'},inplace=True)\n",
    "\n",
    "#Concatenacion por pais\n",
    "\n",
    "df_chi =pd.concat([df_chi_men,df_chi_sem],axis=0,ignore_index=True)\n",
    "df_mex=pd.concat([df_mex_men,df_mex_sem],axis=0,ignore_index=True)\n",
    "df_per=pd.concat([df_per_men,df_per_sem],axis=0,ignore_index=True)\n",
    "df_arg=pd.concat([df_arg_men,df_arg_sem],axis=0,ignore_index=True)\n",
    "\n",
    "#se crea columna asignando pais\n",
    "df_chi['Country'] = 'Chile'\n",
    "df_per['Country'] = 'Peru'\n",
    "df_mex['Country'] = 'Mexico'\n",
    "df_arg['Country'] = df_arg['(L) Retailer'].apply(lambda x: 'Uruguay' if 'Uruguay' in x else 'Argentina')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Funcion que inserta columna vacia \n",
    "def crear_columnas_vacias(df, columnas, posiciones):\n",
    "    for col, pos in zip(columnas, posiciones):\n",
    "        df.insert(loc=pos, column=col, value='')\n",
    "\n",
    "#------------\n",
    "# Mexico        \n",
    "columnas_vacias = [\"vacia1\", \"vacia2\", \"vacia3\", \"vacia4\"]\n",
    "posiciones = [10, 12, 13, 14]\n",
    "crear_columnas_vacias(df_mex, columnas_vacias, posiciones)\n",
    "\n",
    "\n",
    "#------------\n",
    "# Peru\n",
    "columnas_vacias = [\"vacia1\", \"vacia2\", \"vacia3\", \"vacia4\", \"vacia5\", \"vacia6\", \"vacia7\", \"vacia8\"]\n",
    "posiciones = [4, 8, 9, 10, 12, 13, 14, 17]\n",
    "crear_columnas_vacias(df_per, columnas_vacias, posiciones)\n",
    "\n",
    "\n",
    "#------------\n",
    "# Argentina\n",
    "columnas_vacias = [\"vacia1\", \"vacia2\", \"vacia3\", \"vacia4\", \"vacia5\", \"vacia6\", \"vacia7\", \"vacia8\", \"vacia9\"]\n",
    "posiciones = [4, 8, 9, 10, 12, 13, 14, 17, 18]\n",
    "crear_columnas_vacias(df_arg, columnas_vacias, posiciones)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --------- RENAME COLUMNAS -------------------------------\n",
    "col_per=df_per.columns.to_list()\n",
    "col_arg=df_arg.columns.to_list()\n",
    "col_mex=df_mex.columns.to_list()\n",
    "col_chi = df_chi.columns.to_list()\n",
    "\n",
    "dict_renombres_per = {nombre_per: nombre_chi for nombre_per, nombre_chi in zip(col_per, col_chi)}\n",
    "dict_renombres_arg = {nombre_arg: nombre_chi for nombre_arg, nombre_chi in zip(col_arg, col_chi)}\n",
    "dict_renombres_mex = {nombre_mex: nombre_chi for nombre_mex, nombre_chi in zip(col_mex, col_chi)}\n",
    "\n",
    "\n",
    "\n",
    "df_per.rename(columns=dict_renombres_per, inplace=True)  # renombrar las columnas utilizando el diccionario\n",
    "df_mex.rename(columns=dict_renombres_mex, inplace=True)  # renombrar las columnas utilizando el diccionario\n",
    "df_arg.rename(columns=dict_renombres_arg, inplace=True)  # renombrar las columnas utilizando el diccionario\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- df_actualizacion ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "df_actualizacion =pd.concat([df_chi,df_mex,df_arg,df_per],axis=0,ignore_index=True)\n",
    "df_actualizacion = df_actualizacion[df_actualizacion['Date'].str.startswith('2024') & df_actualizacion['Date'].notna()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['202401', '202402', '202403', '202404', '202405', '202406',\n",
       "       '202407', '202408', '202409', '202410', '202411', '202412',\n",
       "       '202413', '202414', '202415', '202416', '202417', '202418',\n",
       "       '202419', '202420', '202421'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tratamiento columna\n"
     ]
    }
   ],
   "source": [
    "#______________________________\n",
    "#---- TRATAMIENTO COLUMNAS ----\n",
    "#______________________________\n",
    "\n",
    "\n",
    "# Eliminar filas con valores NaN en la columna 'Date'\n",
    "df_actualizacion = df_actualizacion.dropna(subset=['Date']) \n",
    "# Convertir a int y luego a str\n",
    "df_actualizacion['Date'] = df_actualizacion['Date'].astype(str) \n",
    "\n",
    "\n",
    "df_actualizacion['(L) Retailer'] =df_actualizacion['(L) Retailer'].str.lower()\n",
    "df_actualizacion['(L) Local'] = df_actualizacion['(L) Local'].str.lower()\n",
    "df_actualizacion['(I) SBU'] =df_actualizacion['(I) SBU'].str.lower()\n",
    "\n",
    "\n",
    "df_actualizacion.rename(columns={'(L) TIENDA FISICA /  ECOMMERCE':'canal_venta'},inplace=True)\n",
    "df_actualizacion['canal_venta'] = df_actualizacion['canal_venta'].str.lower().fillna('vacio').replace('', 'vacio')\n",
    "\n",
    "df_actualizacion['(I) MARCA'] = df_actualizacion['(I) MARCA'].astype(str).str.lower().fillna('vacio').replace('nan','vacio')\n",
    "df_actualizacion['(E) Marca'] = df_actualizacion['(E) Marca'].astype(str).str.lower().fillna('vacio').replace('nan','vacio')\n",
    "\n",
    "df_actualizacion['(I) Código Producto Interno'] = df_actualizacion['(I) Código Producto Interno'].astype(str).str.lower()\n",
    "df_actualizacion['Tipo de dato'] = df_actualizacion['Tipo de dato'].astype(str).str.lower()\n",
    "print(\"tratamiento columna\")\n",
    "\n",
    "#-- Formato numerico\n",
    "df_actualizacion['Venta bruta'] = df_actualizacion['Venta bruta'].str.replace(',', '').astype(float)\n",
    "df_actualizacion['Venta neta']=df_actualizacion['Venta neta'].str.replace(',', '').astype(float)\n",
    "df_actualizacion['Venta costo']=df_actualizacion['Venta costo'].str.replace(',', '').astype(float)\n",
    "df_actualizacion['Unidades vendidas']=df_actualizacion['Unidades vendidas'].str.replace(',', '').astype(float)\n",
    "df_actualizacion['Volumen vendido (Capacidad 1)']=df_actualizacion['Volumen vendido (Capacidad 1)'].str.replace(',', '').astype(float)\n",
    "df_actualizacion['Precio Publico Estimado']=df_actualizacion['Precio Publico Estimado'].str.replace(',', '').astype(float)\n",
    "\n",
    "#--- TRATAMIENTO NULOS\n",
    "def vacios_str(columna):\n",
    "  return columna.fillna('vacio').replace('', 'vacio')\n",
    "\n",
    "columnas_object = list(df_actualizacion.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for columna in columnas_object:\n",
    "  df_actualizacion[columna] = vacios_str(df_actualizacion[columna])\n",
    "\n",
    "def vacios_float(columna):\n",
    "  return columna.fillna(0)\n",
    "columnas_float = list(df_actualizacion.select_dtypes(include=['float']).columns)\n",
    "for columna in columnas_float:\n",
    "  df_actualizacion[columna] = vacios_float(df_actualizacion[columna])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- COLUMNA NUM_FILA  ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "\n",
    "df_actualizacion.reset_index(inplace=True)\n",
    "df_actualizacion.rename(columns={'index': 'num_fila'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.   , 75789.916, 48576.25 , ...,  2206.78 , 16813.55 ,\n",
       "         -92.37 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['Venta neta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- MARCAS  ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "lst_marca= [  \n",
    " 'facom', 'iar expert', 'powers', 'troy-bilt', 'yard machine', 'no usar' ,\n",
    " 'stanley', 'dewalt', 'b+d', 'irwin', 'proto','bostitch', 'fat max', 'porter cable', \n",
    "'lenox', 'craftsman', 'no_usar',  'gridest' \n",
    "]\n",
    "\n",
    "#---------------- ASIGNACION  ---------------------------------------------------\n",
    "\n",
    "\n",
    "df_actualizacion['(I) MARCA'] = np.where((df_actualizacion['(I) MARCA'] =='vacio') & (df_actualizacion['(E) Marca'] != 'vacio'),\n",
    "                               df_actualizacion['(E) Marca'],\n",
    "                               df_actualizacion['(I) MARCA'])\n",
    "\n",
    "df_actualizacion['(I) MARCA'] = df_actualizacion['(I) MARCA'].replace('vacio', 'other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correspondencias = {\n",
    "    'black+decker': ['b/d','black & de', 'black and decker', 'black&decker', 'black & decker', 'black+decker', 'black+deck', 'black + decker', 'b&d', 'b+d', 'black decker', 'black-d', 'black&deck'],\n",
    "    'dewalt': ['dewalt', 'de walt'],\n",
    "    'fatmax': ['stanley fatmax', 'fat max', 'fatmax'],\n",
    "    'bostitch': ['bosch', 'bostitch', 'bostitch office'],\n",
    "    'craftsman':['craftsman','craftman'],\n",
    "    'no usar': ['einhell','sierra','geo','samoa','smart','no usar']\n",
    "}\n",
    "\n",
    "df_actualizacion['(I) MARCA'] = df_actualizacion['(I) MARCA'].apply(lambda x: next((clave for clave, valor in correspondencias.items() if x in valor), x))\n",
    "df_actualizacion['(I) MARCA'] =df_actualizacion['(I) MARCA'].apply(lambda x:'other' if x not in lst_marca else x)\n",
    "df_actualizacion['(I) MARCA'] =df_actualizacion['(I) MARCA'].str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- SBU  ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "df_actualizacion['(I) SBU'] = df_actualizacion['(I) SBU'].fillna('oth').replace(['',' ','no definido','vacio','nan'], 'oth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tienda', 'ecommerce', 'e-commerce', 'moderno', 'vacio'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['canal_venta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- ECOMMERCE TIENDA  ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "lst_canal = 'internet|online|distancia|digital|virtual|ecommerce|e-com'\n",
    "lst_retailer = 'mercado libre|e-comm|ecommerce|mercadolibre|amazon'\n",
    "\n",
    "\n",
    "\n",
    "mask_retailer = df_actualizacion['(L) Retailer'].str.contains(lst_retailer)\n",
    "mask_canal = df_actualizacion['(L) Local'].str.contains(lst_canal)\n",
    "mask_ecom= (mask_retailer|mask_canal)\n",
    "\n",
    "df_actualizacion.loc[mask_ecom, 'canal_venta'] = 'ecommerce'\n",
    "\n",
    "\n",
    "\n",
    "df_actualizacion['canal_venta'] = df_actualizacion['canal_venta'].replace(['vacio','moderno','tradicional','tienda'], 'store')\n",
    "df_actualizacion['canal_venta'] = df_actualizacion['canal_venta'].replace(['ecommerce','e-commerce'], 'e-commerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['store', 'e-commerce'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['canal_venta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- MEXICO THD ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "\n",
    "df_thd = df_actualizacion.loc[(df_actualizacion['Country']=='Mexico')\n",
    "                    & ((df_actualizacion['(L) Retailer'] == 'the home depot') | (df_actualizacion['(L) Retailer'] == 'the home depot e-comm') )\n",
    "                    ]\n",
    "df_thd= df_thd.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_thd['concat_thd'] = df_thd['Tipo de dato'] + df_thd['Date'] +df_thd['(L) Local']+ df_thd['(I) Código Producto Interno']+ df_thd['canal_venta']\n",
    "df_thd['concat_thd_ecom'] = df_thd['Tipo de dato'] + df_thd['Date'] +df_thd['(L) Local']+ df_thd['(I) Código Producto Interno']+ 'e-commerce'\n",
    "#df_thd['countif'] = df_thd.groupby('concat_thd_ecom')['concat_thd'].transform('count')\n",
    "counts = df_thd['concat_thd'].value_counts()\n",
    "df_thd['countif'] = df_thd['concat_thd_ecom'].map(counts)\n",
    "df_thd['concat_update'] = df_thd['Tipo de dato']+df_thd['Country'] + df_thd['Date'] +df_thd['(L) Retailer']\n",
    "df_thd['countif'] = df_thd['countif'].replace(np.nan, 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#contador= 0\n",
    "lst_concat= df_thd['concat_thd'].to_list()\n",
    "lst_concat_ecom= df_thd['concat_thd_ecom'].to_list()\n",
    "\n",
    "def actualizar_venta_neta(row, lst_concat):\n",
    "    if ((row['canal_venta'] == 'store') and (row['countif'] >= 1)):\n",
    "        row_index = lst_concat.index(row['concat_thd_ecom'])\n",
    "        row['Venta neta'] = row['Venta neta'] - df_thd.at[row_index, 'Venta neta']\n",
    "    else:\n",
    "        row['Venta neta'] = row['Venta neta']\n",
    "    return row\n",
    "\n",
    "df_thd = df_thd.apply(lambda row: actualizar_venta_neta(row, lst_concat) , axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['store', 'e-commerce'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_thd['canal_venta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------- ACTUALIZACION TDH\n",
    "df_thd.drop(['concat_thd','concat_thd_ecom', 'countif'],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df_merged = pd.merge(df_actualizacion[['num_fila']], df_thd[['num_fila', 'Venta neta']], on='num_fila', how='left')\n",
    "df_actualizacion['Venta neta'] = df_merged['Venta neta'].fillna(df_actualizacion['Venta neta'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['store', 'e-commerce'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['canal_venta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- MEXICO COPPEL ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "\n",
    "ruta_coppel = r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\VS Code Datamind\\Data\\Coppel\\precios_historico_coppel.csv'    \n",
    "    \n",
    "df_coppel_precios = pd.read_csv(ruta_coppel)\n",
    "df_coppel_precios['concat'] = df_coppel_precios['MODELO']+df_coppel_precios['P PUBLICO']\n",
    "df_coppel_precios.drop_duplicates(subset=['concat'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_coppel = df_actualizacion.loc[(df_actualizacion['Country']=='Mexico')\n",
    "                    & ((df_actualizacion['(L) Retailer'] == 'coppel b')  )\n",
    "                    ]\n",
    "df_coppel= df_coppel.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#-------------------------------- COPPEL --------------------------------------------\n",
    "# Convertir columnas a string y minúsculas\n",
    "\n",
    "df_coppel_precios['MODELO'] = df_coppel_precios['MODELO'].astype(str).str.lower()\n",
    "df_coppel_precios['año'] = df_coppel_precios['año'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Crear nuevas columnas\n",
    "df_coppel['year_modelo'] = df_coppel['Date'].str[:4] + df_coppel['(I) Código Producto Interno']\n",
    "df_coppel_precios['year_modelo'] = df_coppel_precios['año'] + df_coppel_precios['MODELO']\n",
    "df_coppel_precios = df_coppel_precios.drop_duplicates(subset='year_modelo')\n",
    "\n",
    "\n",
    "\n",
    "# Realizar la búsqueda\n",
    "df_coppel = df_coppel.merge(df_coppel_precios[['year_modelo', 'P PUBLICO']], on='year_modelo', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Convertir precios a float64\n",
    "df_coppel['P PUBLICO'] = df_coppel['P PUBLICO'].str.replace(',', '').astype(np.float64)\n",
    "\n",
    "\n",
    "# Calcular la venta neta\n",
    "df_coppel['Venta neta'] = df_coppel['Unidades vendidas'] * df_coppel['P PUBLICO']\n",
    "\n",
    "\n",
    "#elimino columna para poder concatener al df_historico\n",
    "df_coppel.drop(['year_modelo', 'P PUBLICO'],axis=1,inplace=True)\n",
    "\n",
    "#-------------- ACTUALIZACION DF_ACTUALZICION\n",
    "df_merged = pd.merge(df_actualizacion[['num_fila']], df_coppel[['num_fila', 'Venta neta']], on='num_fila', how='left')\n",
    "df_actualizacion['Venta neta'] = df_merged['Venta neta'].fillna(df_actualizacion['Venta neta'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- COLUMNA UPDATE PARA ACTUALIZAR DF_HISTORICO ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    " \n",
    "df_actualizacion['concat_update'] = df_actualizacion['Country']+df_actualizacion['Tipo de dato']+df_actualizacion['Date']\n",
    "df_actualizacion['concat_update'] = df_actualizacion['concat_update'].str.lower().str.strip()\n",
    "\n",
    "\n",
    "df_actualizacion['concat_update_meli_amz'] = df_actualizacion['Country']+df_actualizacion['(L) Retailer']+df_actualizacion['Tipo de dato']+df_actualizacion['Date']\n",
    "df_actualizacion['concat_update_meli_amz'] = df_actualizacion['concat_update'].str.lower().str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['store', 'e-commerce'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['canal_venta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#---------------- CALENDAR ---------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "from datetime import datetime as dt\n",
    "def get_date_from_year_week(year_week):\n",
    "    year = int(year_week[:4])\n",
    "    week = int(year_week[4:])\n",
    "    # Obtener la fecha del primer día de la semana\n",
    "    first_day_of_week = datetime.strptime(f'{year}-W{week-1}-1', '%Y-W%W-%w')\n",
    "    # Agregar 1 día para obtener la fecha del lunes de esa semana\n",
    "    date = first_day_of_week + timedelta(days=1)\n",
    "    # Formatear la fecha en el formato deseado\n",
    "    return date.strftime('%m-%d-%Y')\n",
    "\n",
    "def get_date_from_year_month(year_month):\n",
    "    year = int(year_month[:4])\n",
    "    month = int(year_month[4:])\n",
    "    # Obtener la fecha del primer día del mes\n",
    "    date = datetime(year, month, 1)\n",
    "    # Formatear la fecha en el formato deseado\n",
    "    return date.strftime('%m-%d-%Y')\n",
    "\n",
    "\n",
    "# Aplicar las funciones lambda al dataframe\n",
    "df_actualizacion['Fecha']=''\n",
    "df_actualizacion = df_actualizacion.fillna('')\n",
    "df_actualizacion.loc[(df_actualizacion['Tipo de dato'] == 'sem') & (df_actualizacion['Fecha'] == ''), 'Fecha'] = df_actualizacion.loc[df_actualizacion['Tipo de dato'] == 'sem', 'Date'].apply(get_date_from_year_week)\n",
    "df_actualizacion.loc[(df_actualizacion['Tipo de dato'] == 'men') & (df_actualizacion['Fecha'] == ''), 'Fecha'] = df_actualizacion.loc[df_actualizacion['Tipo de dato'] == 'men', 'Date'].apply(get_date_from_year_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_________________________________________________\n",
    "#---------- HOLDER ---------------------------\n",
    "#_________________________________________________\n",
    "df_actualizacion['Holder']=''\n",
    "df_actualizacion['Holder']=''\n",
    "notacion_holder = {\n",
    "    'Mercado Libre': ['mercadolibre', 'mercado libre multivende', 'mercado libre spiral', 'mercado libre spiral ar'],\n",
    "   'Sodimac': ['sodimac', 'sodimac mexico', 'sodimac argentina', 'sodimac uruguay', 'sodimac peru'],\n",
    "    'Amazon': ['amazon mx'],\n",
    "    'Walmart': ['walmart', 'walmart mexico', 'walmart argentina'],\n",
    "    'Coppel': ['coppel b'],\n",
    "    'The Home Depot': ['the home depot e-comm', 'the home depot'],\n",
    "    'Easy': ['easy argentina']\n",
    "}\n",
    "notacion_pais = {\n",
    "    'Mexico': 'MX',\n",
    "    'Chile': 'CH',\n",
    "    'Argentina': 'AR',\n",
    "    'Uruguay': 'URU',\n",
    "    'Peru': 'PE'\n",
    "}\n",
    "\n",
    "lst = ['mercado libre', 'sodimac', 'walmart', 'the home depot', 'easy', 'amazon']\n",
    "\n",
    "def holder(row):\n",
    "    pais = notacion_pais.get(row['Country'], row['Country'])\n",
    "    retailer = notacion_holder.get(row['(L) Retailer'], row['(L) Retailer']).lower()\n",
    "    local = row['(L) Local'].lower()\n",
    "\n",
    "    if pais == 'CH' and retailer == 'mercado libre' and local == 'fcom fcom':\n",
    "        retailer = 'falabella'\n",
    "    elif pais == 'CH' and retailer == 'mercado libre' and local == 'paris':\n",
    "        retailer = 'paris'\n",
    "\n",
    "    if retailer in lst:\n",
    "        return retailer +' '+ pais\n",
    "    else:\n",
    "        return retailer\n",
    "df_actualizacion['Holder'] = df_actualizacion.apply(holder, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['store', 'e-commerce'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['canal_venta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ausin', 'construmart', 'easy', 'falabella', 'ferretek',\n",
       "       'imperial', 'mercado libre multivende', 'mts', 'oviedo',\n",
       "       'pernos kim', 'sodimac', 'villar hermanos', 'walmart', 'amazon mx',\n",
       "       'coppel b', 'la comer', 'liverpool', 'mercado libre spiral',\n",
       "       'office depot', 'officemax', 'sears', 'sodimac mexico', 'soriana',\n",
       "       'the home depot', 'the home depot e-comm', 'walmart mexico',\n",
       "       'easy argentina', 'la anonima', 'mercado libre spiral ar',\n",
       "       'sodimac argentina', 'sodimac uruguay', 'walmart argentina',\n",
       "       'promart', 'sodimac peru'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['(L) Retailer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ausin', 'construmart', 'easy CH', 'falabella', 'ferretek',\n",
       "       'imperial', 'mercado libre multivende', 'mts', 'oviedo',\n",
       "       'pernos kim', 'sodimac CH', 'villar hermanos', 'walmart CH',\n",
       "       'amazon mx', 'coppel b', 'la comer', 'liverpool',\n",
       "       'mercado libre spiral', 'office depot', 'officemax', 'sears',\n",
       "       'sodimac mexico', 'soriana', 'the home depot MX',\n",
       "       'the home depot e-comm', 'walmart mexico', 'easy argentina',\n",
       "       'la anonima', 'mercado libre spiral ar', 'sodimac argentina',\n",
       "       'sodimac uruguay', 'walmart argentina', 'promart', 'sodimac peru'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['Holder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ausin', 'construmart', 'easy CH', 'falabella', 'ferretek',\n",
       "       'imperial', 'mercado libre multivende', 'mts', 'oviedo',\n",
       "       'pernos kim', 'sodimac CH', 'villar hermanos', 'walmart CH',\n",
       "       'amazon mx', 'coppel b', 'la comer', 'liverpool',\n",
       "       'mercado libre spiral', 'office depot', 'officemax', 'sears',\n",
       "       'sodimac mexico', 'soriana', 'the home depot MX',\n",
       "       'the home depot e-comm', 'walmart mexico', 'easy argentina',\n",
       "       'la anonima', 'mercado libre spiral ar', 'sodimac argentina',\n",
       "       'sodimac uruguay', 'walmart argentina', 'promart', 'sodimac peru'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['Holder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#___________________________\n",
    "#VENTA USD \n",
    "#___________________________\n",
    "ruta_fx_rate = r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Shares Information for Projects\\FX Rate\\FX_Rate.xlsx'\n",
    "\n",
    "df_actualizacion['fecha_my'] = pd.to_datetime(df_actualizacion['Fecha']).dt.strftime('%m%Y')\n",
    "\n",
    "df_fx_rate = pd.read_excel(ruta_fx_rate)\n",
    "df_fx_rate.drop_duplicates(subset=['Fecha', 'Country'], inplace=True) # Agregar inplace=True para modificar el dataframe original\n",
    "df_fx_rate['Fecha'] = pd.to_datetime(df_fx_rate['Fecha'])\n",
    "df_fx_rate['fecha_my'] = df_fx_rate['Fecha'].dt.strftime('%m%Y')\n",
    "df_fx_rate = df_fx_rate[df_fx_rate['Country'].isin(df_actualizacion['Country']) \n",
    "                          & df_fx_rate['fecha_my'].isin(df_actualizacion['fecha_my'])]\n",
    "\n",
    "df_actualizacion = pd.merge(df_actualizacion, df_fx_rate[['fecha_my', 'Country', 'Adjusted Rate']], on=[\"fecha_my\", \"Country\"], how='left')\n",
    "try:\n",
    "    df_actualizacion['Venta_neta_usd'] = df_actualizacion['Venta neta'] / df_actualizacion['Adjusted Rate']\n",
    "except:\n",
    "    df_actualizacion['Venta_neta_usd'] = df_actualizacion['Venta neta']\n",
    "\n",
    "df_actualizacion['Venta_neta_usd']=df_actualizacion['Venta_neta_usd'].fillna(0)\n",
    "\n",
    "df_actualizacion.drop(['fecha_my', 'Adjusted Rate'],axis=1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['store', 'e-commerce'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion['canal_venta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carga df_actualizacion_historico local\n",
      "Agrego la informacion nueva al actualizacion_historico\n",
      "guarda archivo local\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Se toma la version local, para optimizar el procesamiento\n",
    "\n",
    "#---df_historico\n",
    "#path_df_historico = r'C:\\Users\\SSN0609\\Documents\\Dashboards LAG PC LOCAL\\Data Flow\\Datamind\\Data Flow\\df_datamind_historico.csv'\n",
    "#df_historico = pd.read_csv(path_df_historico,dtype=str)\n",
    "\n",
    "\n",
    "#df_actualizacion_historico\n",
    "path_df_actualizacion_historico=r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\Data Flow\\datamind_actualizacion_historico.csv'\n",
    "\n",
    "df_actualizacion_historico=pd.read_csv(path_df_actualizacion_historico,dtype=str,header=0)\n",
    "df_actualizacion_historico['canal_venta'] = df_actualizacion_historico['canal_venta'].str.replace('Ecommerce','e-commerce')\n",
    "\n",
    "print(\"carga df_actualizacion_historico local\")\n",
    "\n",
    "\n",
    "#--- ACTUALIZAR DF_ACTUALIZACION_HISTORICO\n",
    "#Con el fin de eliminar del historico la informacion que se desea actualizar\n",
    "\n",
    "#Obtener los índices de las filas que deben ser eliminadas de df_actualizacion_historico\n",
    "indices_a_eliminar_actualizacion = df_actualizacion_historico[df_actualizacion_historico['concat_update'].isin(df_actualizacion['concat_update'])].index\n",
    "\n",
    "#Eliminar los registros del df_actualizacion\n",
    "df_actualizacion_historico = df_actualizacion_historico.drop(indices_a_eliminar_actualizacion, axis=0)\n",
    "\n",
    "#Agrego la informacion nueva al actualizacion_historico\n",
    "df_actualizacion_historico = pd.concat([df_actualizacion_historico, df_actualizacion], ignore_index=True)\n",
    "\n",
    "print(\"Agrego la informacion nueva al actualizacion_historico\")\n",
    "#Obtener los índices de las filas que deben ser eliminadas\n",
    "#Con el fin de que no se agrege info que ya esta en el df_historico, pues este no se actualiza\n",
    "\n",
    "#indices_a_eliminar = df_actualizacion_historico[df_actualizacion_historico['concat_update'].isin(df_historico['concat_update'])].index\n",
    "#Eliminar los registros del df_actualizacion\n",
    "#df_actualizacion_historico = df_actualizacion_historico.drop(indices_a_eliminar, axis=0)\n",
    "\n",
    "#print(\"verifica que no se agrege info que ya esta en el df_historico, pues este no se actualiza\")\n",
    "\n",
    "#______________________________\n",
    "#--- GUARDAR\n",
    "#______________________________\n",
    "\n",
    "\n",
    "#--- DRIVE\n",
    "\n",
    "#Guardar el df_historico como archivo CSV en la ruta deseada\n",
    "ruta_drive_archivo_actualizacion_csv = r'C:\\Users\\SSN0609\\OneDrive - Stanley Black & Decker\\Dashboards LAG\\Data Flow\\Datamind\\Data Flow\\datamind_actualizacion_historico.csv'\n",
    "df_actualizacion_historico.to_csv(ruta_drive_archivo_actualizacion_csv, index=False)\n",
    "\n",
    "print(\"guarda archivo local\")\n",
    "#--- LOCAL\n",
    "#Guardar el df_historico como archivo CSV en la ruta deseada\n",
    "ruta_local_archivo_actualizacion_csv = r'C:\\Users\\SSN0609\\Documents\\Dashboards LAG PC LOCAL\\Data Flow\\Datamind\\Data Flow\\datamind_actualizacion_historico.csv'\n",
    "df_actualizacion_historico.to_csv(ruta_local_archivo_actualizacion_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['e-commerce', 'store'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actualizacion_historico['canal_venta'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
